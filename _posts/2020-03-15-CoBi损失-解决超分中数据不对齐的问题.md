---
title: Loss function for image transformation with Non-aligned Data 
date:  2020-03-15 20:34:13 +0800
category: computer-vision
tags: deep-learning super-resolution loss
excerpt: 在图像生成中解决数据未对齐的损失函数
comment: True
mathjax: True
---



### 1. Contextual Loss$^{[1]}$

#### Contextual Loss 定义

为了度量图像 $x$ 和目标图像 $y$ 之间的距离，我们首先将图像转化为一系列的特征点，如：
假设特征点数量为 $N$, 则特征点的集合为：
$X = \lbrace x_0, x_1, \cdots, x_N \rbrace$ 和 $Y = \lbrace y_0, y_1, \cdots, y_N\rbrace$。

1). 定义特征点 $x_i$ 和 $y_j$ 之间的余弦距离为：

$$
d_{ij} = 1 - \frac{(x_i - \mu_y)(y_j - \mu_y)}{||x_i-\mu_y||_2 ||y_j-\mu_y||_2} \quad s.t. \quad \mu_y = \frac{1}{N}\sum_jy_j
$$

2). 距离规范化。文中认为，如果 $\forall k \ne j, d_{ij} \ll d_{ik}$ 则认为 $x_i$ 和 $y_j$ 是相似的。

$$
\hat{d}_{ij} = \frac{d_{ij}}{\min_k d_{ik} + \epsilon} \in [1, +\infty)
$$

如果 $d_{ik}$ 值最小，表示 $x_i$ 与 $y_j$ 之间的距离最近， 则上述规范化操作会使得 $\hat{d_{ik}} = 1$, 而其他点之间的距离 $\hat{d_{ik}}$ 会随着与原始距离之间的差异变大而变大。当 $d_{ij} \ll d_ik$ 时， $\hat{d}_{ij} \rightarrow +\infty $。

3) 取幂操作。 我们对距离取幂，限制范围为 [0,1]

$$
w_{ij} = e^{\frac{1 - \hat{d_{ij}}}{h}} \in [0, 1]
$$

4) 归一化。

$$
\text{CX}_{ij} = \frac{w_{ij}}{\sum_k w_{ik}}
$$

上述计算出的特征点 $x_i$ 和 $y_j$ 之间的距离 $CX_{ij}$ 满足，

1. 如果 $Y$ 中当且仅有一个点与 $x_i$ 之间的距离比其他点都远远要小，则$\text{CX}_{ij} = 1$。此时匹配的最好。

2. 如果$Y$ 中有 $m$ 个点与 $x_i$ 之间的距离比其他点都远远要小，则对于这 $m$ 个点：$CX_{ij} = \frac{1}{m} \quad s.t. \quad j \in set\{m\}$, 而对于这 $m$ 个点：$\text{CX}_{ij} = 0 \quad s.t. \quad j \notin set\{m\}$。

3. 如果 $Y$ 中的点与 $x_i$ 之间的距离都差不多，则 $\text{CX}_{ij} = \frac{1}{N} \rightarrow  0$, 此时可以认为 $x_i$ 与 $Y$ 匹配失败。

5) 定义图像之间所有特征点之间的距离

$$\text{CX}(x, y) = \text{CX}(X, Y) = \frac{1}{N} \sum_j \max_i \text{CX}_{ij}$$

容易发现：$\text{CX}(x,y)$ 随着图像 $x$ 和图像 $y$ 之间的相似程度增大而增大。

所以损失函数定义为：

$$
\begin{aligned}
\mathcal{L}_\text{CX}(x, y) &= - \log(\text{CX}(x, y)) \\
&= \frac{1}{N} \sum_j \min_{i\in{1 \cdots m}} -\log(\text{CX}_{ij})
\end{aligned}$$


**[Pytorch 代码实现](https://github.com/z-bingo/Contextual-Loss-PyTorch)** 


```
//求距离 1)
raw_distance = Contextual_Loss._create_using_dotP(I_features, T_features)
//求相对距离 2)
relative_distance = Contextual_Loss._calculate_relative_distance(raw_distance)
//取幂 3)
exp_distance = torch.exp((self.b - relative_distance) / self.h)
//归一化 4)
contextual_sim = exp_distance / torch.sum(exp_distance, dim=-1, keepdim=True)
//求CX_ij 5)
CS = torch.max(torch.max(contextual_sim, dim=1)[0], dim=1)[0]
//CX求距离
CX_loss = torch.mean(-torch.log(CS))
```

<font color="blue"><b>缺点：$L_1$ 和 $L_2$ 考虑对应位置的像素，然后计算两者之间的距离，对于不对齐的数据无法处理。而 CX Loss 完全不考虑位置信息，在全图中搜索相似特征点。这种机制只考虑特征信息，而完全不考虑空间之间的关系；在某些应用上表现并不好。</b></font>

代码实现注意：假设 VGG 中间层的输出特征维度为 $\text{NCHW}$. 则取特征向量为 $(C, h_i, w_j) \in \mathcal{R}^{C * 1 * 1}$。其中 $i,j$ 表示特征向量的空间位置。 

### 2. Cobi Loss$^{[2]}$

论文中提出如果不对上述特征向量的空间位置 $i$ 和 $j$ 进行限制，CX 损失会出现大量的点匹配 (双向匹配) 失败。
所以论文中提出将 空间位置 损失 加入到 CX 损失中。

$$
\begin{aligned}
\mathcal{L}_\text{CoBi}(x, y) &= \frac{1}{N} \sum_j \min_{i\in{1 \cdots m}} (\mathbb{D_{ij}} + \mathbb{w_s} \mathbb{D'}_{ij}) 
\end{aligned}
$$

其中 $\mathbb{D_{ij}} = -\log(CX_{ij})$，$\mathbb{D'}_{ij} = \|\|(h_i, w_i) - (h_j, w_j)\|\|_2$。$(h_i,w_i)$ 和 $(h_j, w_j)$ 表示特征 $x_i$ 和 $y_j$ 之间的空间位置距离。


**[Pytorch代码实现](https://github.com/ceciliavision/zoom-learn-zoom/blob/887acbeb7cdb2961417e5ad580bee1c4da9f9421/CX/CSFlow.py#L115)** 


<center> <font size="5"> <b>Reference</b> </font> </center>

[1]. The Contextual Loss for Image Transformation with Non-Aligned Data.

[2]. Zoom to Learn, Learn to Zoom. 