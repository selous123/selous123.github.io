---
title: 卷积操作说明
date:  2020-03-19 13:34:13 +0800
category: computer-vision
tags: deep-learning convolution
excerpt: 详细介绍神经网络中的卷积操作(a)——持续更新
comment: True
mathjax: True
---

卷积作为神经网络中最基础的模块，其重要性也是不言而喻的。其作用相当于学习到特定的卷积核 filter 通过卷积操作提取到输入的特定的特征，然后利用这些特征去做接下来的任务。下面我们详细介绍卷积操作的过程。


博客动图来源：https://github.com/vdumoulin/conv_arithmetic

### 1. 卷积操作

**基本卷积操作**： 假设输入和卷积核为：

$$
x = \left(
    \begin{aligned}
    3,3,2,1,0 \\
    0,0,1,3,1 \\
    3,1,2,2,3 \\
    2,0,0,2,2 \\
    2,0,0,0,1
    \end{aligned}
    \right) \in\mathcal{R}^{5 \times 5}; \quad
\text{kernel} = \left(
    \begin{aligned}
    0,1,2 \\
    2,2,0 \\
    0,1,2
    \end{aligned}
    \right) \in\mathcal{R}^{3 \times 3}
$$

则输出$o = x \otimes \text{kernel} \in \mathcal{R}^{3\times 3}$; 则基本卷积过程如下图所示为：


<center><img src="https://selous123.github.io/assets/img/blog-conv/conv.gif" width="60%" height="auto"/>

<span>图1. 卷积操作示意图.</span></center>

<font color="blue">缺点：从上述卷积操作的结果可以发现 $5 \times 5$ 的特征图谱 $x$ 经过 $3 \times 3$ 的卷积操作后，输出结果为 $3 \times 3$ 的特征 $o$。</font> 在实际应用中，特征图谱会随着卷积层数的增加而不断减小。这样的情况是不利于深度网络的。
其中输入和输出之间的大小关系为：

$$
\text{O}_\text{shape} = \text{I}_\text{shape} - \text{kernel\_size} + 1
$$

**Padding操作**： 所以基于上述操作我们就会在输入特征 $x$ 四周填充 0 元素，保证输入和输出大小不变。如下为 $\text{Padding} = 1$ 的操作示意图：

<center><img src="https://selous123.github.io/assets/img/blog-conv/padding.gif" width="60%" height="auto"/>

<span>图2. Padding操作示意图.</span></center>

数学公式为：

$$
\text{O}_\text{shape} = \text{I}_\text{shape} - \text{kernel\_size} + 2 * \text{Padding} + 1
$$

<font color="red">此时卷积操作输出的大小和原始特征图谱大小相同。常见 $\text{kernel}\_\text{size}$ 和 $\text{padding}$ 组合为：$(3,1),(5,2),(7,3)$ </font>

**Stride步长操作**：
步长参数表示卷积核每次移动的距离，默认步长为1。具体操作如下图所示：

<center><img src="https://selous123.github.io/assets/img/blog-conv/p.gif" width="60%" height="auto"/>

<span>图3. stride参数操作示意图.</span></center>

输入输出的数学公式表达为：

$$
\text{O}_\text{shape} = \lceil\frac{\text{I}_\text{shape} - \text{kernel\_size} + 2 * \text{Padding} + 1}{\text{stride}}\rceil
$$


### 2. 卷积操作参数量和计算量

<center><img src="https://selous123.github.io/assets/img/blog-conv/conv.png" width="80%" height="auto"/>

<span>图3. 卷积层操作示意图.</span></center>

所以卷积操作的参数量为：

$$
\text{Params} = C_\text{in} * C_\text{out} * K * K 
$$

卷积操作的计算量（Flops）为：

$$
\begin{aligned}
\text{Flops}_\text{mul} &= C_\text{in} *  K * K * H_\text{out} * W_\text{out} * C_\text{out} \\
\text{Flops}_\text{add} &= (C_\text{in} * K * K - 1) * H_\text{out} * W_\text{out} * C_\text{out} \\
\end{aligned}
$$

### 3. 卷积操作感受野计算

1. 最后一层（卷积层或池化层）输出特征图感受野的大小等于卷积核的大小。

2. 第i层卷积层的感受野大小和第i层的卷积核大小和步长有关系，同时也与第（i+1）层感受野大小有关。

3. 计算感受野的大小时忽略了图像边缘的影响，即不考虑padding的大小。

关于感受野大小的计算是自下向上计算，计算公式为：

$$
\text{RF}_i = ( \text{RF}_{i+1} - 1 ) * \text{stride} + \text{ksize}_i
$$


示例如图所示：
<center><img src="https://selous123.github.io/assets/img/blog-conv/effective_field.png" width="70%" height="auto"/>

<span>图. 卷积感受野示意图.</span></center>


### 4. 空洞卷积 (Dilated convolution)

<center><img src="https://selous123.github.io/assets/img/blog-conv/dilation.gif" width="60%" height="auto"/>

<span>图. 空洞卷积操作示意图.</span></center>

### 5. 装置卷积 (transpose convolution)

<center><img src="https://selous123.github.io/assets/img/blog-conv/no_padding_strides_transposed.gif" width="60%" height="auto"/>

<span>图. 反转卷积（逆卷积）操作示意图.</span></center>


### 6. Group 分组卷积

<center><img src="https://selous123.github.io/assets/img/blog-conv/gconv.png" width="90%" height="auto"/>

<span>图. group卷积操作示意图.</span></center>

Group卷积参数量：

$$
\begin{aligned}
\text{Params} &= K * K * \frac{C_\text{in}}{G} * C_\text{out} \\
\text{Flops} &= K * K * \frac{C_\text{in}}{G} * \frac{C_\text{out}}{G} * H * W * G
\end{aligned}
$$

故 分组卷积的的参数量 $\text{Params}$ 和 计算量 $\text{Flops}$ 都是原始卷积的 $\frac{1}{G}$。


**当分组卷积中参数 $G$ 取值为 $C_\text{in}$ 时， 分组卷积就成为了 Xception 结构中 的depth-wise convolution。**

<font color="blue">但是这种卷积方式输出的特征图谱，只和对应的特征图谱有关，输出特征图中所包含的信息太少。</font> 所以一般会加一个point-wise convolution。如图所示。

<center><img src="https://selous123.github.io/assets/img/blog-conv/xception.png" width="90%" height="auto"/>

<span>图. Xception卷积操作示意图.</span></center>


$$
\begin{aligned}
\text{Params} &= K * K * 1  * C_\text{in} + 1 * 1 * C_\text{in} * C_\text{out} \\
\text{Flops} &= K * K * 1 * C_\text{in} *  H_\text{out} * W_\text{out} + C_\text{in} * 1 * 1 * H_\text{out} * W_\text{out} * C_\text{out}
\end{aligned}
$$



