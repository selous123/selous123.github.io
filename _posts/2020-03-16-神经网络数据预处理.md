---
title: 深度神经网络数据预处理
date:  2020-03-19 23:43:13 +0800
category: deep-learning
tags: deep-learning pre-processing
excerpt: 深度学习基础
mathjax: true
---

### 0. 归一化对神经网络训练的影响

#### 0.1 特征存在量级的差异

对于线性函数 $f(x) = w_1x_1 + w_2x_2 + b$。如果 $x_1$ 和 $x_2$ 的取值存在量级的差异，比如 $x_1$ 是十的级别，但是 $x_2$ 是万的级别。损失函数对 $w_1$ 求偏导是和 $x_1$ 线性相关的。 对 $w_2$ 求偏导是和 $x_2$ 线性相关的。这样就导致了两个维度的参数下降速度不一致的现象。明显 $w_2$ 更新要比 $w_1$ 快很多。也就可能会导致 $w_1$ 先到达最低点，而需要等待 $w_1$ 更新，最终导致更新过程不断震荡。 

#### 0.2 特征不是0均值的


##### 0.2.1. **解释1（存疑）：**

如下图所示：

<center><img src="https://selous123.github.io/assets/img/blog-preprocess/zpath.png" width="50%" height="auto"/>
<span>z-path 示意图</span>
</center>

A点为起始点，B点为最优点。
如果数据的特征数值不是有正有负，那么对于 $w$ 的每个分量的导数，就会和 $x$ 分量的符号是一致的。**假设 所有的特征值都是正的，那么损失函数对所有 $w$ 分量的偏导符号就是一样的（要么全为正，要么全为负）。此时就不能一次从A点更新到B点，需要从C点过渡，使得更新变复杂。**

<font color="blue">存疑点：当有 $m$ 个点时，比如线性回归 $f(x) = w x + b$，损失函数为$L = (f(x) - y)^2$。 则其导数形式为：</font>

$$\frac{\partial L}{\partial w} = \frac{1}{m}\sum_{i=1}^m 2 (f(x^{(i)}) - y^{(i)}) * x^{(i)}$$

<font color="blue">即使特征 $x$ 所有样本中都是正的，但是由于$w$导数的方向同时也会和 $f(x^{(i)}) - y^{(i)}$ 的方向相关。所以 $w$ 的导数的方向并不是确定的。 也就是说明对于上述的解释并不存在！</font>

**存疑点解惑：不是对于所有的模型都需要归一化$^{[3]}$, 比如逻辑回归。**

<font color="red"> 说得通的点 (z-path 是用来解释对于一个样本，如果所有特征值均为正的，那么会收敛变慢)：例如如果对于某一个点 (SGD)，</font>

$$
\begin{aligned}
\frac{\partial L}{\partial w_1} =  2 (f(x) - y) * x_1 \\
\frac{\partial L}{\partial w_2} =  2 (f(x) - y) * x_2
\end{aligned}
$$

<font color="red"> 如果$x_1$ 和 $x_2$ 都是正的，那么导数方向会是一样的。于是我们希望 $x_1$ 和 $x_2$ 的值是有正有负的！（这是激活函数的问题）。</font> 同样该解释对于 $m$ 个点在均方误差时也是不适用的，但是对于有些损失函数，比如 $-\log$ 损失就适用了，因为对于所有数据点，该项导数的方向是确定的。

#### 0.2.2. 解释2： 待补充

### 1. 缩放归一化

将所有的特征都归一化到 $[0, 1]$ 或者 $[-1, 1]$ 之间

$$
\hat{x}^{(i)} = \frac{x^{(i)} - \min_k x^{(k)}}{\max_k x^{(k)} - \min_k x^{(k)}}
$$

其中 $\max_k x^{(k)} 和 \min_k x^{(k)}$ 表示所有样本中特征 $x$ 的最大值和最小值。


### 2. 标准归一化

将每一维的特征归一化为标准正态分布 $\mathcal{N}(0,1)$。

$$
\begin{aligned}
\mu &= \frac{1}{m}\sum_{i=1}^m x^{(i)} \\
\sigma^2 &= \frac{1}{m} \sum_{i=1}^m (x^{(i)} - \mu)^2 \\
\hat{x}^{(i)} &= \frac{x^{(i)} - \mu}{\sigma} \in \mathcal{N}(0,1)
\end{aligned}
$$

每一维特征都服从标准正态分布。

### 3. 白化

白化是一种重要的与处理方式，用来降低输入数据特征之间的冗余性，输入数据经过白化处理后，特征之间相关性低，并且所有特征具有相同的方差。

白化具有2个目的：1. 使数据的不同维度去相关；2. 数据每个维度的方差为1。

<font color="red">白化的优势：假设训练数据是图像，由于图像中相邻像素之间具有很强的相关性，所以用于训练时输入是冗余的。白化的目的就是降低输入的冗余性。</font>

#### 3.1. PCA 白化

通过PCA将数据X降维以后得到Z，可以看出Z中每一维都是独立的(去相关)，这时再除Z中每一维对应的方差，得到每一维方差均为1。如果 $k < n$，就是PCA降维，如果 $k = n$，则降低特征间相关性。这里的 $k$ 是指PCA处理过后的特征数。

#### 3.2. ZCA 白化

$$X_{ZCA} = UX_{PCA}$$

只是在PCA白化的基础上做了一个旋转操作，使得白化之后的数据更加接近原始数据。

ZCA白化首先通过PCA去除了各个特征相关性，然后是输入特征具有单位方差，此时得到PCA白化后的结果，然后再将数据旋转回去，得到ZCA白化结果


#### PCA 基础知识

1. 去中心化。

2. 求特征之间的协方差矩阵。

3. 计算协方差矩阵的特征值 $\lambda_i$ 和 特征向量。

4. 选出特征值前$k$大的特征向量，将其特征值对应的特征向量作为列向量，构成转移矩阵 $\mathbb{R}^{n * k}$。

5. 将数据 $\mathbb{R}^{m * n}$ 映射到转移矩阵 $\mathbb{R}^{n * k}$ 中得到PCA变换后的数据： $\mathbb{R}^{m * k}$。

<center><font size="5"> <b>Reference</b> </font></center>

[1]. 白化操作详解 https://www.cnblogs.com/jfdwd/p/11240987.html

[2]. 数据归一化 https://zhuanlan.zhihu.com/p/69439309

[3]. 有些模型不需要归一化 https://blog.csdn.net/pipisorry/article/details/52247379