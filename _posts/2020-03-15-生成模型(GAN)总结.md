---
title: 基于 Wasserstein 距离主问题的优化
date:  2020-03-15 11:19:13 +0800
category: computer-vision 
tags: deep-learning GAN Apersonal-project
excerpt: 生成网络介绍(a)
comment: True
mathjax: True
---

### 1. 前言知识
常用的分布度量函数：

1). The Total Variation (TV) distance:

$$
TV(P_r, P_g) = \sup_X |P_r(X) - P_g(X)|
$$

2). The Kullback-Leibler (KL) divergence:

$$KL(P_r || P_g) = \int_x log(\frac{P_r(x)}{P_g(x)})P_r(x)dx$$

**KL散度特性：**

1. $KL(P_r \|\| P_g) \ge 0$ 当两个分布完全一致的时候，KL散度为0

2. 非对称 $KL(P_r \|\| P_g) \ne KL(P_g \|\| P_r)$

3. KL 散度不满足距离的定义，不满足三角不等式，即 不满足$KL(P_r\|\|P_t) \le KL(P_r\|\|P_g) + KL(P_g\|\|P_t)$  

4. 相对熵定义来源： $KL(P_r\|\|P_g) = H(P_r\|\|P_g) - H(P_r) = -P_r \log P_g + P_r \log P_r = P_r \log \frac{P_r}{P_g}$

3). The Jenson-Shannon (JS) divergence: 令$P_M = \frac{1}{2}(P_r+P_g)$ 为两个分布的混合分布。

$$
\begin{aligned}
JS(P_r, P_g) &= \frac{1}{2}KL(P_r || P_M) + \frac{1}{2}KL(P_g || P_M) \\
& = \frac{1}{2}\int_x log(\frac{P_r(x)}{P_M(x)})P_r(x)dx+ \frac{1}{2} \int_x log(\frac{P_g(x)}{P_M(x)})P_g(x)dx
\end{aligned}
$$

**JS散度特性：**
1. $JS(P_r\|\|P_g) \ge 0$ 当两个分布完全一致的时候，JS散度为0 (存疑)

2. 对称 $JS(P_r \|\| P_g) = JS(P_g \|\| P_r)$

 <font color="blue">KL散度和JS散度都以一个致命的缺点：</font>如果两个分布 $P_r$, $P_g$ 离得很远，<font color="red">测度（分布之间交集的集合大小）为 $0$ 的时候，</font>那么KL散度值是没有意义的，而JS散度在这个情况下值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为 $0$。梯度消失了。

4). The Earth Mover / Wasserstein distance:

$$
W(P_r, P_g) = \inf_{\gamma \in \Pi(P_r, P_g)} \mathbb{E}_{(x,y) \in \gamma}||x - y||
$$

其中 $\Pi(P_r, P_g)$ 是以 $P_r$ 和 $P_g$ 为边际分布（marginal distribution）的联合分布（joint distribution).

Wasserstein 距离的定义说明：Wasserstein距离的求解要点在于如何优化得出最优的联合分布 $\Pi_{\text{opt}}(P_r, P_g)$，使得在该分布下，两个分布之间的距离差的期望最小。而求得的最小期望即为两个分布之间的 Wasserstein 距离

**直观上可以把 $\mathbb{E}_{(x,y)\in\gamma} \|\|x−y\|\|$理解为在γ这个路径规划下把土堆P1挪到土堆P2所需要的消耗。而Wasserstein距离就是在最优路径规划下的最小消耗。所以Wesserstein距离又叫Earth-Mover距离。**

### 1. GAN

Vanilla GAN 本身是一个博弈问题，对于样本 $X$ 的真实分布 $P_d$和生成器生成样本的$G(z)$的分布 $P_g$ , 我们通过定义 判别器 D 去判断给定样本是来自真实分布还是生成分布。训练过程中，我们训练 $G$ 生成足够相似的样本，使得 $D$ 无法判断，而训练 $D$ 尽可能判别出两者的差异。这就是GAN的博弈过程。定义二分类交叉熵损失即为:

$$
\min_G\max_D L(G, D) = \min_G\max_D(\mathbb{E}_{x \sim P_d}[\log(D(x))] + \mathbb{E}_{z \sim P_g}[\log(1 - D(G(z)))])
$$

数学推导：

$$
\begin{aligned}
L(G, D) &= \mathbb{E}_{x \sim P_d}[\log(D(x))] + \mathbb{E}_{x \sim P_g}[\log(1 - D(x)] \\
&= \int_x \big(p_d(x) \log(D(x)) + p_g(x) \log(1-D(x))\big)dx
\end{aligned}
$$

<font color="red">下面我们通过损失函数的导数，求出最优的分类器。</font> 假设 $\hat{x} = D(x)$, $p_d(x) = A$, $p_g(x) = B$, 则：

$$
\begin{aligned}
f(\hat{x}) &= A\log(\hat{x}) + B\log(1-\hat{x}) \\
\frac{\text{d}f(\hat{x})}{\text{d}\hat{x}} &= A * \frac{1}{\hat{x}} - B * \frac{1}{1-\hat{x}} \\
&= \frac{(1-\hat{x})A - \hat{x}B}{\hat{x}(1 - \hat{x})} \\
&= \frac{A - (A + B)\hat{x}}{\hat{x}(1 - \hat{x})}
\end{aligned}
$$

令导数为0， 可以求得：

$$
D^*(x) = \frac{A}{A+B} = \frac{p_d(x)}{p_d(x) + p_g(x)}
$$

则：

$$
\begin{aligned}
L(G, D^*) &= \int_x(p_d(x)\log(\frac{p_d(x)}{p_d(x) + p_g(x)}) + p_g(x)\log(\frac{p_g(x)}{p_d(x) + p_g(x)}))dx \\
&=2 JS(p_d||p_g) - 2\log2
\end{aligned}
$$

所以对于GAN来说，优化 $G$ 函数相当于是在优化分布之间的JS散度。但是由于JS散度。根据第一节中的描述，我们知道，如果分布$p_r$ 和 $p_g$ 相差太远，分布之间的JS散度为常数，导数为0，就导致了网络结构无法正常训练，这就造成了 vanilla GAN 的训练困难：

<font color="blue">vanilla GAN 的问题：</font>

1. 难以训练: 梯度消失 （JS散度造成的）

2. 模式坍塌 （未知原因）

### 2. WGAN

为了改进GAN模型， WGAN引入 Wasserstein 距离来度量两个分布的距离。<font color="red">Wasserstein 距离的优点： 它相对KL散度与JS散度具有优越的平滑特性，这个距离在两个分布之间不重叠时也是连续的，理论上可以解决梯度消失问题。</font>

#### 2.1. 举例说明KL散度/JS散度/Wasserstein距离之间的差异：

假设定义两维 $\mathcal{R}^2$ 分布，真实分布 $P_r \in \mathcal{U}[0,z]$, 其中$z \in \mathcal{U}[0,1] $；生成分布 $P_g \in \mathcal{U}[\theta, z]$, 同样的 $z \in \mathcal{U}[0,1] $。 我们画出分布图如下：

<center><img src="https://selous123.github.io/assets/img/blog-gan/distribution.png" width="60%" height="auto"/>

<span>图. $P_r$ 和 $P_g$ 的示意图.</span></center>

a. KL散度

因为对于KL散度，当存在点 $(x,y)$ 满足 $P(x,y)>0$ and $Q(x,y) = 0$时，$KL(P\|\|Q) = +\infty$。所以当 $\theta \ne 0$ 时，$P_r$ 和 $P_g$ 分布之间不存在重合点，则 $KL(P_r\|\|P_g) = +\infty$；当 $\theta = 0$ 时，$P_r$ 和 $P_g$ 重合，此时 $KL(P_r\|\|P_g) = 0$。

$$
KL(P_r, P_g) =  \left\{ 
    \begin{aligned}
    +\infty \qquad  &\text{if}\quad \theta \ne 0 \\
    0 \qquad &\text{if}\quad\theta = 0
    \end{aligned}
    \right.
$$

b. JS散度

如果$\theta \ne 0$, 则对于任何$P_r(x,y) \ne 0$的位置，$P_g(x,y) = 0$，于是 $P_M(x,y) =\frac{1}{2}(P_r(x,y) + P_g(x,y)) = \frac{1}{2}P_r(x,y)$，同样的对于任何 $P_g(x,y) \ne 0$的位置同样有 $P_M(x,y) = \frac{1}{2}P_g(x,y)$。则：

$$
JS(P_r, P_g) = \left\{
\begin{aligned}
\log2 \qquad  &\text{if}\quad \theta \ne 0 \\
0 \qquad &\text{if}\quad\theta = 0
\end{aligned}
\right.
$$

c. Wasserstein 距离

而对于wasserstein距离，两个分布之间只存在分布关系，所以之间的最小移动代价，就是直接平移过去就可以，于是两个分布之间的 Wasserstein 距离为 $\theta$。

$$
W(P_r, P_g) = \theta
$$

通过上述例子我们可以发现 Wasserstein 距离是可以度量两个不相交分布之间的距离的，而且是光滑的，处处可到。

<font color="red">我们重申Wassertein距离在度量分布距离问题中的优势： </font>
我们令$P_r$为固定的数据分布，$Z$ 是一个随机的高斯变量，然后通过生成器网络$g_\theta$输出生成的分布 $P_\theta = g_\theta(Z)$。

1. 如果 $g$ 函数关于 $\theta$ 是连续的，那么 $W(P_r, P_\theta)$ 关于 $\theta$ 也是连续的。

2. KL散度和JS散度不能满足条件1。


#### 2.2. 如何计算 Wasserstein 距离

Wasserstein 距离定义：

$$
W(P_r, P_g) = \inf_{\gamma \in \Pi(P_r, P_g)} \mathbb{E}_{(x,y) \in \gamma}||x - y||
$$

但是不幸的是，精确计算Wasserstein距离是很难的，因为我们无法知道原始数据和生成数据的真实分布，所以WGAN的原始论文中提出了对偶的方法来解决Wasserstein距离的难以计算的问题。

对偶问题定义：

$$
W(P_r, P_x) = \sup_{||f||_L \le 1} \mathbb{E}_{x\sim P_r}[f(x)] - \mathbb{E}_{x\sim P_g}[f(x)]
$$

其中 $f$ 函数 是一个映射；我们希望找到一个最优的映射 $f$ 满足 1. 1-Lipschitz 性质；2. 上述表达式所有可能取值的上界。

#### 2.3. 对偶形式推导

#### 2.4. 与 Vanilla GAN 训练方式的比较

1). 在 GAN 中，我们最小化的二分类的交叉熵损失函数为：

$$
\frac{1}{m} \sum_{i=1}^{m} \log(D(x_i)) + \frac{1}{m} \sum_{i=1}^{m} (1 - \log(D(g(z_i)))
$$

所以判别器 D 的输出要经过激活函数 sigmoid 限制在$[0,1]$。

而在WGAN中，损失函数为：

$$
\frac{1}{m} \sum_{i=1}^{m} f(x_i) - \frac{1}{m} \sum_{i=1}^{m} (f(g(z_i))
$$

$f$ 函数并没有这种限制。所以网络架构中需要把激活函数去掉。

2). 通过GAN中的推到我们知道，如果 每次迭代 都把 D 训练到最优，那么 G 相当于在优化分布之间的 JS 散度，而对于JS散度来说，当分布差异大的时候，就会变成常数，导数为0，使得网络训练失败，所以在GAN的训练中，一般生成器迭代n次，而判别器迭代1次； 在WGAN中，因为wasserstein 距离是处处可导的，所以我们每次迭代都可以精确计算wasserstein距离，同时为了加快训练速度，一般先迭代 $f$ 训练 n 次，然后生成器迭代 1 次。

上述描述基本解释了GAN训练不稳定的问题，但是关于模式坍塌并没有给出很好的解释，原始论文中只提及一句：**In no experiment did we see evidence of mode collapse for the WGAN algorithm.**

### 3. Primal WGAN

为了对 WGAN 有更深层次的理解，我们的工作关注于如何直接优化Wassertein 距离，而不是转化为对偶问题进行优化。

首先我们重新复习一下 Wasserstein 距离的定义：

$$
W(P_r, P_g) = \inf_{\gamma \in \Pi(P_r, P_g)} \mathbb{E}_{(x,y) \in \gamma}||x - y||
$$

解决该问题的难点在于**如何定义联合分布函数 $\Pi(P_r, P_g)$**。

在EWD（经验Wasserstein距离）中，为了直接计算Wasserstein 距离，将所有采样到的样本定义为 $\frac{1}{n}$，也就是$P_r = \frac{1}{n}, P_g = \frac{1}{n}$。因此 Wasserstein 距离可以定义为：

$$
\begin{aligned}
W(P_r, P_g) &= \inf_{\gamma \in \Pi(P_r, P_g)} \mathbb{E}_{(x,y) \in \gamma}||x - y|| \\
s.t.& \left\{
\begin{aligned}
\sum_i\pi_{ij} = \frac{1}{n} \\
\sum_j\pi_{ij} = \frac{1}{n} \\
\pi_{ij} >0 \\
\end{aligned}
\right.
\end{aligned}
$$

这里，$\pi_^\*{ij}=\frac{1}{n}$ 对应于将生成的数据点$y_j$传输到实际数据点$x_i$的距离，在$x_i$ 到所有y点的传输代价已经明确的前提下，目标是最小化平均传输成本；这就表明每个生成的数据点与真实样本是一一对应。因此，求解最优匹配 $\pi^\*$ 可以形式化为线性分配问题（LAP）。我们利用匈牙利算法求解最优匹配$\pi^*$，并在GPU上实现，使算法更快、更省时。

如下图所示，横向表示真实样本 $X \in [x_1, \cdots, x_8]$， 纵向表示生成样本 $Y \in [y_1, \cdots, y_8]$。黑色表示数据点 $x$ 与数据点 $y$ 匹配。

<center><img src="https://selous123.github.io/assets/img/blog-gan/distribution.png" width="60%" height="auto"/>

<span>图. mini-batch 带来的问题.</span></center>


如上图所示，假设样本量为8个，min-batch的m取值为4，则mini-batch 计算出的Wassertein距离 匹配方式与真实数据分布的方式有比较大的差异，导致了计算产生偏差，也就是mini-batch随机采样的方式会导致主问题训练不稳定，无法收敛。

最终我们计算出两者的偏差为：

$$
\begin{aligned}
|B_M| &= \frac1n \sum_{i=1}^m \sum_{\substack{j=s_i \\ \pi^*_i(j)\neq \pi^*(j)}}^{ib}\big|c(x_j,y_{\pi^*_i(j)}) - c(x_j,y_{\pi^*(j)})\big| \\
&\le \frac1n \sum_{i=1}^m \sum_{\substack{j=s_i \\ \pi^*_i(j)\neq \pi^*(j)}}^{ib} c(y_{\pi^*_i(j)},y_{\pi^*(j)})
\end{aligned}
$$

我们发现假设 $\forall \pi^*_i(j) \neq \pi^*(j), c(y_{\pi^*_i(j)},y_{\pi^*(j)}) \rightarrow 0$ 则 $\|B_M\| \rightarrow 0$.

**上述证明说明如果mini-batch采样的分布服从数据集 $X$ 的分布，则偏差$B_M$趋于零。在这种情况下，MBWD的优化相当于EWD的优化，模型是稳定的。**

通过对WGAN 主问题的研究我们可以总结出WGAN的性质：

1). 模型中基本不会出现模式坍塌的问题，因为一一匹配的性质保证了所有抽样的样本都会参与计算。

2). Mini-batch随机采样的训练方式会导致主问题WGAN训练不稳定。在对偶问题中的表现就是会 Catastrophic forgetting（灾难性遗忘）——GAN在训练中，会逐渐遗忘前面batch中学习到的数据分布—— 目前ICML2019 上就有一篇基于DPP采样代替随机采样额工作来提高GAN的训练$^{[3]}$。

### 4. 总结

GAN 是从14年开始一直被大家广泛研究的问题。虽然WGAN提出很严密的数学证明，但是在实际应用的过程中WGAN的效果并没有比GAN有很大的提升，所以GAN的优秀还是被大家广泛认可的。

从WGAN主问题中的研究中我们发现，如果只使用单一的生成器去生成样本，那么生成的样本会与真实的样本很相似。**所以GAN是否能生成它未见过的样本，依然是存疑的。** 

1). 在GAN的工作中一般使用插值图像表示GAN的泛化性能，那么生成插值图像是否就可以算作是它未见过的样本？

2). 从PWGAN的研究中我们发现，精确的计算Wasserstein 距离并不能提升GAN网络的泛化性能。所以我认为 GAN 优秀的创作能力很大程度上是判别器D（GAN）或者critic函数（WGAN）的不确定性带来的。 

3). GAN 的思想 也已经被应用到各大领域，大部分结合的工作也都是基于Discriminator在做文章，这也同样说明 Discriminator 是GAN中最精华的思想。


<center> <font size="5"><b>Reference</b></font> </center>

[1]. https://vincentherrmann.github.io/blog/wasserstein/

[2]. https://www.alexirpan.com/2017/02/22/wasserstein-gan.html

[3]. Elfeki, M. et al. GDPP: Learning Diverse Generations using Determinantal Point Processes. In ICML 2019
