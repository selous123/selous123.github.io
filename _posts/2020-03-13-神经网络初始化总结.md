---
title: 深度神经网络中常用初始化函数总结
date:  2020-03-13 20:25:13 +0800
category: deep-learning
tags: deep-learning initialization
excerpt: 深度学习基础(a)
mathjax: true
comment: true
---

好的初始化方法通常只是为了增快学习的速度(加速收敛)，但在某些网络结构中也能够提高准确率.

## 1. 网络初始化方式
### 1.1. 标准高斯分布
模型参数通过标准高斯分布进行参数初始化，如：$\mathcal N(0, 0.01)$

### 1.2. Xavier 初始化$^{[1]}$
原始文章是基于全连接网络
初始化方式为：

$$ 
\begin{aligned} 
W \sim \mathcal U(-k , k) \\
s.t. \quad k = \sqrt\frac{6}{{n_{\text{in}} + n_{\text{out}}}} 
\end{aligned}
$$

其中$n_{\text{in}}$ 与 $n_{\text{out}}$ 分别表示全连接层的输入维度和输出维度。

[关于xavier初始化的理论分析和实验论证](https://blog.csdn.net/weixin_35479108/article/details/90694800)[需要补充]
### 1.3. Kaiming 初始化$^{[2]}$
该初始化方式是针对ReLU激活函数提出的一种基于Xavier相同假设的改进版初始化方式。
数学表达为：

$$
\begin{aligned}
W \sim \mathcal N (0, \sqrt{\frac{4}{n_\text{in}+ n_\text{out}}})
\end{aligned}
$$

<font color="red">注意：在pytorch中的官方实现中，对应不同激活函数，初始化中的常数$\text{gain}$都是不同的。</font> 

## 2. 常见问题

1.) Pytorch框架中，默认的卷积层和全连接层的网络初始化方式是什么？

a. 全连接层通过标准均匀分布进行参数初始化 (kaiming均匀分布)：

$$
\begin{aligned}
& W \sim \mathcal{U}(-\sqrt{k}, \sqrt{k}) \\
& \text{where} \quad k = \frac{\text{gain}}{C_\text{out}}
\end{aligned}
$$

```
stdv = 1. / math.sqrt(self.weight.size(1))
self.weight.data.uniform_(-stdv, stdv)
if self.bias is not None:
    self.bias.data.uniform_(-stdv, stdv)
```

b. 2D卷积层通过标准正态分布进行参数初始化 (kaiming正态分布)：

$$
\begin{aligned}
W &\sim \mathcal{N}(-\sqrt{k}, \sqrt{k}) \\
\text{where} \quad k &= \frac{\text{gain}}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}
\end{aligned}
$$

b.1. Pytorch 中 如何将 Xavier 如何应用到卷积层----[代码实现](https://pytorch.org/docs/stable/_modules/torch/nn/init.html#xavier_uniform_)

首先计算 $\text{c\_in}$ 和 $\text{c\_out}$

```
c_in = n_feature_maps_in * receptive_field_height * receptive_field_width
c_out = n_feature_maps_out * receptive_field_height * receptive_field_width
```

然后：

$$
W \sim \mathcal U(-\sqrt{\frac{6}{\text{c\_in} + \text{c\_out}}}, \sqrt{\frac{6}{\text{c\_in} + \text{c\_out}}})
$$


对于不同层的初始化方式，我们可以从pytorch官方代码中查到：https://github.com/pytorch/pytorch/tree/master/torch/nn/modules


c. pytorch中自定义网络初始化方式

```
def weights_init(m):
    if isinstance(m, nn.Conv2d):
        torch.nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            torch.nn.init.xavier_uniform_(m.bias)

model.apply(weights_init)
```


<center> <font size="5"><b>Reference</b></font> </center>

[1]. Understanding the difficulty of training deep feedforward neural networks by Xavier Glorot, Yoshua Bengio in AISTATS 2010.

[2]. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,Kaiming He