---
title: 深度神经网络中常用损失函数总结
date:  2020-03-13 17:12:13 +0800
category: deep-learning
tags: deep-learning loss
excerpt: 深度学习基础(a)
mathjax: True
---

### 1. 交叉熵损失(分类任务)

**交叉熵定义**：$H(p, q) =- p \log q$

**交叉熵损失：**

$$
L = \frac{1}{m} \sum_{i=1}^m L_i = \frac{1}{m} \sum_{i=1}^m \sum_{c=1}^t-y^i_{c} \log(p^i_{c})
$$

**二元交叉熵：**

$$
\begin{aligned}
L &= \frac{1}{m}\sum_{i=1}^m\sum_{c=0}^1-y_{c}^i\log(p^i_{c}) \\
  &= -\frac{1}{m}\sum_{i=1}^m y^i_{0}\log(p^i_{0}) + y^i_{1}\log(p^i_{1}) \\
  &= -\frac{1}{m}\sum_{i=1}^m y^i \log(p^i) + (1-y^i)\log(1-p^i)
\end{aligned}
$$


**交叉熵函数的求导：**

<center><img src="https://selous123.github.io/assets/img/blog-loss/gradient.png" width="700" height="auto"/>

<span>图. 求导示意图.</span></center>

$$
\begin{aligned}
\frac{\partial J(\theta)}{\partial \theta_j} &= \frac{\partial J(\theta)}{\partial p^i} * \frac{\partial p^i}{\partial z^i} * \frac{\partial z^i}{\partial \theta_j} \\
&=(p^i - y^i) * x_j^i\\
\frac{\partial J(\theta)}{\partial \theta_j} &=\frac{1}{m}\sum_{i=1}^m (p^i - y^i) * x_j^i \quad \text{For m samples.} 
\end{aligned}
$$

### 2. NLLLoss(负对数似然函数)

$$L = \frac{1}{m} \sum_{i=1}^m \sum_{c=1}^t -y_c^i\log(\text{output}_c^i)$$

和 交叉熵损失的 表达式一样， 差别在于 交叉熵损失 默认的 $p_c^i$ 是经过softmax的结果。

所以 :
```
This criterion (i.e., CrossEntropy) combines nn.LogSoftmax() and nn.NLLLoss() in one single class.
```

### 3. $L_p$ 损失函数

$$
L(\text{output}, y) = \frac{1}{m}\sum_{i=1}^m \sum_{c=1}^t |\text{output}^{(i)}_c - y^{(i)}_c|_p
$$

### 4. 基于 Margin 的损失函数 (人脸识别应用)
 

### 5. 常见问题

#### 1). 数据类别不均衡怎么解决？

**数据方面：** a). 获取更多的少量样本数据。
b). 重采样，上采样，下采样，合成数据。
c). 数据增强；对少量数据加噪声或者和augmentation.

<font color="blue">基于采样的方法会改变训练样本的分布，由于测试集上的分布仍然是不均衡的，所以在均衡数据集上训练的结果一般在测试集上表现都比较差。</font> <font color ="red">在实际项目中，我们一般要保证训练集和测试集的分布相同。</font>

**评价指标方面：** 不要只看Accuracy， 指标高并不能代表问题解决的很好。比如99:1的二分类数据集，即使完全识别不出来，精度也有99%。所以需要根据实际的任务关注不同的指标。如 Precision（预测为正样本的集合中预测正确的概率）和Recall（标签为正样本的集合中预测正确的概率）。相关指标定义如下：

$$
\begin{aligned}
\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{NP}}  \\
\text{Recall} = \frac{\text{TP}}{\text{TP}+\text{TN}}
\end{aligned} 
$$

ROC曲线和PR曲线（纵坐标/横坐标）：a).（实际负样本中被预测为正样本的概率，Recall） b).（Precision, Recall）. 

**模型架构：** a). 集成学习，将多数类样本抽样成若干子集与少量类样本联合起来训练多个基网络；投票得出结果。
b). 将任务转化成异常检测的任务。

**损失方面：** 对损失进行加权，让样本量少的类对应损失项加大权重。

#### 2). 神经网络的损失函数为什么是非凸的?

**判断函数是否为凸函数：**

对于一元函数$f(x)$，我们可以通过其二阶导数$f''(x)$ 的符号来判断。如果函数的二阶导数总是非负，即$f''(x) \ge 0$ ，则$f(x)$是凸函数。

对于多元函数$f(X)$，我们可以通过其Hessian矩阵（Hessian矩阵是由多元函数的二阶导数组成的方阵）的正定性来判断。如果Hessian矩阵是半正定矩阵，则$f(X)$为凸函数


#### 3). Loss 函数不减反增怎么解决？

a). 模型结构存在问题，是不是代码有问题。
b). 权重初始化方案有问题。无脑使用xavier 和 he norm即可。
c). 正则化——$L_1, L_2$ 和 Dropout——过度，导致模型拟合能力不够。
d). 选择合适的优化器（无脑Adam）和learning rate。
e). 未进行归一化。

#### 4). 为什么用交叉熵做损失函数？

a). 交叉熵损失相比于均方误差比较陡，梯度大，更新速度快。

b). 求导计算简单。

#### 5) Pooling层如何求导？

<center><img src="https://selous123.github.io/assets/img/blog-loss/grad_pooling.png" width="700" height="auto"/>

<span>图. pooling操作求导示意图.</span></center>


#### 6) 线性回归导数推导过程？（$L_2$损失）

**线性回归问题定义：** 假设数据输入特征为 $x = (x_0, x_1, x_2)$ 其中 $x_0 = 1$。回归值为 $Y$。 定义线性回归模型参数为 $\theta=(\theta_0, \theta_1, \theta_2)$。则对于样本 $x^{(i)}$ 模型表达式为：

$$H_\theta(x^{(i)}) = \theta^T x^{(i)} $$

**推导线性回归的损失函数：** 假设预测值和真实值之间存在噪声，而噪声 $\epsilon$ 符合均值为0(方差为 $\theta^2$)[存疑] 的高斯分布，则：

$$y^{(i)} = \Theta^T x^{(i)} + \epsilon^{(i)}$$

$$
p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(\epsilon^i)^2}{2\sigma^2}}
$$

于是

$$
p(y^{(i)}|x^{(i)}, \theta) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}}
$$

有了数据和分布函数，我们就可以对 $\theta$ 进行估计。常用方法如极大似然估计：

假设所有样本是独立同分布的，则似然函数为：

$$
\begin{aligned}
L(\theta) &= \prod_{i=1}^m p(y^{(i)}|x^{(i)}, \theta) = \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}} \\
\text{ln} L(\theta) &= \text{ln}\prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}}\\ &= \sum_{i=1}^m \text{ln} \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}} \\
&=\text{m} * \text{ln} \frac{1}{\sqrt{2\pi}\sigma} -\frac{1}{\sigma^2} \frac{1}{2} \sum_{i=1}^m (y^{(i) } - \theta^T x^{(i)})^2
\end{aligned} 
$$

于是对于线性回归模型，最大化似然估计也就的等价于最小化平方误差：

$$\min J(\theta) = \frac{1}{2} \sum_{i=1}^m (y^{(i) } - \theta^T x^{(i)})^2 = \frac{1}{2} (X\theta - Y)^T(X\theta - Y)$$

其中 $X \in \mathcal{R}^{m \times n}, \theta \in \mathcal{R}^{n \times 1}, Y \in \mathcal{R}^{m \times 1}$ 

$$
\begin{aligned}
\nabla_\theta{J(\theta)} &= \nabla_\theta(\frac{1}{2} (\theta^TX^T - Y^T)(X\theta - Y)) \\
&=\nabla_\theta \frac{1}{2} (\theta^TX^TX\theta - \theta^TX^TY - Y^TX\theta+Y^TY)\\
&=\frac{1}{2}(2X^TX\theta - X^TY - (Y^TX)^T)\\
&=X^TX\theta - X^TY
\end{aligned}
$$

当且仅当 $(X^TX)^{-1}$ 存在时，$\theta$ 存在解析解为 $\theta = (X^TX)^{-1}X^TY$

如果不存在，则使用梯度下降的方法计算，其导数为：

$$
\begin{aligned}
J(\theta) &= \frac{1}{2m}  \sum_{i=1}^m (y^{(i) } - \theta^T x^{(i)})^2 \\
\frac{\partial J(\theta)}{\partial\theta_j} &= \frac{1}{m} \sum_{i=1}^m (\theta^T x^{(i)} - y^{(i) })x_j^{(i)}
\end{aligned}
$$

然后可以选择不同的梯度下降的优化算法进行优化。


<center><font size="5"> <b>Reference</b> </font></center>

[1]. 人脸识别任务中的基于Margin的损失函数，https://blog.csdn.net/u013841196/article/details/89923075

[2]. 线性回归推导过程，https://blog.csdn.net/u011573853/article/details/97500354