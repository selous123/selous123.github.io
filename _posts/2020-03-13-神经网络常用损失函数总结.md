---
title: 深度神经网络中常用损失函数总结
date:  2020-03-13 17:12:13 +0800
category: deep-learning
tags: deep-learning loss
excerpt: 深度学习基础(a)
mathjax: True
---

### 1. 交叉熵损失(分类任务)

**交叉熵定义**：$H(p, q) =- p \log q$

**交叉熵损失：**

$$
L = \frac{1}{m} \sum_{i=1}^m L_i = \frac{1}{m} \sum_{i=1}^m \sum_{c=1}^t-y^i_{c} \log(p^i_{c})
$$

**二元交叉熵：**

$$
\begin{aligned}
L &= \frac{1}{m}\sum_{i=1}^m\sum_{c=0}^1-y_{c}^i\log(p^i_{c}) \\
  &= -\frac{1}{m}\sum_{i=1}^m y^i_{0}log(p^i_{0}) + y^i_{1}log(p^i_{1}) \\
  &= -\frac{1}{m}\sum_{i=1}^m y^i log(p^i) + (1-y^i)log(1-p^i)
\end{aligned}
$$


**交叉熵函数的求导：**

<center><img src="https://selous123.github.io/assets/img/blog-loss/gradient.png" width="700" height="auto"/>

<span>图. 求导示意图.</span></center>

$$
\begin{aligned}
\frac{\partial J(\theta)}{\partial \theta_j} &= \frac{\partial J(\theta)}{\partial p^i} * \frac{\partial p^i}{\partial z^i} * \frac{\partial z^i}{\partial \theta_j} \\
&=(p^i - y^i) * x_j^i\\
\frac{\partial J(\theta)}{\partial \theta_j} &=\frac{1}{m}\sum_{i=1}^m (p^i - y^i) * x_j^i \quad \text{For m samples.} 
\end{aligned}
$$

### 2. NLLLoss(负对数似然函数)

<!-- $$L = \frac{1}{m} \sum_{i=1}^m \sum_{c=1}^C -log(y_c^i)$$ -->



### 3. 常见问题

#### 1). 数据类别不均衡怎么解决？

**数据方面：** a). 获取更多的少量样本数据。
b). 重采样，上采样，下采样，合成数据。
c). 数据增强；对少量数据加噪声或者和augmentation.

<font color="blue">基于采样的方法会改变训练样本的分布，由于测试集上的分布仍然是不均衡的，所以在均衡数据集上训练的结果一般在测试集上表现都比较差。</font> <font color ="red">在实际项目中，我们一般要保证训练集和测试集的分布相同。</font>

**评价指标方面：** 不要只看Accuracy， 指标高并不能代表问题解决的很好。比如99:1的二分类数据集，即使完全识别不出来，精度也有99%。所以需要根据实际的任务关注不同的指标。如 Precision（预测为正样本的集合中预测正确的概率）和Recall（标签为正样本的集合中预测正确的概率）。相关指标定义如下：

$$
\begin{aligned}
\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{NP}}  \\
\text{Recall} = \frac{\text{TP}}{\text{TP}+\text{TN}}
\end{aligned} 
$$

ROC曲线和PR曲线（纵坐标/横坐标）：a).（实际负样本中被预测为正样本的概率，Recall） b).（Precision, Recall）. 

**模型架构：** a). 集成学习，将多数类样本抽样成若干子集与少量类样本联合起来训练多个基网络；投票得出结果。
b). 将任务转化成异常检测的任务。

**损失方面：** 对损失进行加权，让样本量少的类对应损失项加大权重。

#### 2). 神经网络的损失函数为什么是非凸的?

**判断函数是否为凸函数：**

对于一元函数$f(x)$，我们可以通过其二阶导数$f''(x)$ 的符号来判断。如果函数的二阶导数总是非负，即$f''(x) \ge 0$ ，则$f(x)$是凸函数。

对于多元函数$f(X)$，我们可以通过其Hessian矩阵（Hessian矩阵是由多元函数的二阶导数组成的方阵）的正定性来判断。如果Hessian矩阵是半正定矩阵，则$f(X)$为凸函数


#### 3). Loss 函数不减反增怎么解决？

a). 模型结构存在问题，是不是代码有问题。
b). 权重初始化方案有问题。无脑使用xavier 和 he norm即可。
c). 正则化——$L_1, L_2$ 和 Dropout——过度，导致模型拟合能力不够。
d). 选择合适的优化器（无脑Adam）和learning rate。
e). 未进行归一化。

#### 4). 为什么用交叉熵做损失函数？

a). 交叉熵损失相比于均方误差比较陡，梯度大，更新速度快。

b). 求导计算简单。

#### 5) Pooling层如何求导？

<center><img src="https://selous123.github.io/assets/img/blog-loss/grad_pooling.png" width="700" height="auto"/>

<span>图. pooling操作求导示意图.</span></center>