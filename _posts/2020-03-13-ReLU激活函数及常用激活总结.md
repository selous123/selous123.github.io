---
title: 深度神经网络中常用激活函数总结
date:  2020-03-13 12:43:13 +0800
category: deep-learning
tags: deep-learning activation
excerpt: 深度学习基础
mathjax: true
---

**激活函数**：神经网络中的非线性模块，增加网络的表达和学习能力。如果神经网络中没有激活函数，那么每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当。


### Sigmoid
Sigmoid激活函数是深度神经网络中最初提出来，其数学表达形式为：

$$\sigma(x) = \frac{1}{1+e^{-x}} $$
其中x为激活函数的输入。
<center><img src="https://selous123.github.io/assets/img/blog-activationfunc/sigmoid.png" width="300" height="200"/></center>

Sigmoid激活函数导数：
$$
\begin{aligned}
\dfrac{d}{dx} \sigma(x) &= \dfrac{d}{dx} \left[ \dfrac{1}{1 + e^{-x}} \right] \\
&= \dfrac{d}{dx} \left( 1 + \mathrm{e}^{-x} \right)^{-1} \\
&= -(1 + e^{-x})^{-2}(-e^{-x}) \\
&= \dfrac{e^{-x}}{\left(1 + e^{-x}\right)^2}
\end{aligned}
$$

**注：Sigmoid函数的梯度在x=0的位置取的最大值，为0.25**

优点：可以将神经网络中的输出通过非线性函数映射到0~1之间，增加网络的非线性表达能力。

缺点：
1. 容易梯度消失，因为$\lim_{n->\infty}(0.25)^n = 0$，而且两端导数接近为$0$。
2. 激活函数的输出值不是zero-centered, [知乎相关解释](https://www.zhihu.com/question/57194292/answer/278445184)。
3. 幂计算相对复杂。

### Tanh
Tanh激活的提出解决Sigmoid的缺点2, 其数学表达式为：
$$ \sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
<center><img src="https://selous123.github.io/assets/img/blog-activationfunc/tanh.png" width="300" height="200"/></center>

但是sigmoid激活中的缺点1和3仍然存在。
### ReLU
ReLU的数学表达式为：

$$\sigma(x) = max(0, x)$$
<center><img src="https://selous123.github.io/assets/img/blog-activationfunc/relu.png" width="300" height="200"/></center>

如上图可以看出，relu激活函数的导数满足当$x<0$时，其导数为0，当$x>0$时，导数为1.

优点:
1. 解决了梯度消失的问题(在正区间中)
2. 计算速度快
3. 收敛快。<font color='red'>为什么收敛快？</font>

缺点：
1. 输出不是zero-centered
2. 在网络中会出现死结点，比如如果某神经元的输出为负值，那么反向传播到激活层时，其导数就为0，该神经元在该样本下就不会发生更新。

**基于ReLU的改进：**
针对ReLU的缺点2, 改进有Leakly ReLU 和Parametric ReLU。其数学表达式为：
$$\sigma(x) = max(\alpha x, x)$$
在LReLU中，$\alpha$一般取很小的值，如$\alpha=0.01$，而PReLU中$\alpha$是可以学习的。


### 常见问题
#### 1. ReLU的负半轴导数都是0，这部分产生的梯度消失怎么办？


#### 2. ReLU激活，导致网络出现Dead Neuron的原因是什么？如何避免？
导致该问题的原因主要有(1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。
#### 3. zero-centered问题？为什么通过zero-centered可以加快网络收敛速度？
<center><img src="https://selous123.github.io/assets/img/blog-activationfunc/zero-centered.jpg" width="400" height="200"/></center>
深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。

**为什么要zero-centered?**

Reference:

博客：
[1]. https://blog.csdn.net/tyhj_sf/article/details/79932893

[2]. https://blog.csdn.net/weixin_41417982/article/details/81437088


